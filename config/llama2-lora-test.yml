# Quick test config: 7 L1 examples, 1 epoch

base_model: meta-llama/Llama-2-7b-chat-hf
sequence_len: 2048

load_in_8bit: false
flash_attention: true

adapter: lora
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_linear: true
lora_modules_to_save:
  - embed_tokens
  - lm_head

datasets:
  - path: data.jsonl
    ds_type: json
    type:
      field_instruction: instruction
      field_input: input
      field_output: output
      format: |-
        [INST] {instruction}

        {input} [/INST]

special_tokens:
  bos_token: '<s>'
  eos_token: '</s>'
  unk_token: '<unk>'

val_set_size: 0.0

seed: 42

optimizer: adamw_torch
learning_rate: 0.0002
lr_scheduler: cosine
warmup_steps: 5
gradient_accumulation_steps: 1
micro_batch_size: 4

dataset_prepared_path: last_run_prepared
output_dir: ./lora-out

logging_steps: 1
eval_steps: 0
save_strategy: 'no'
num_epochs: 1

wandb_project: blocksworld-planning-test

bf16: auto
fp16: false
tf32: false
deepspeed: /workspace/axolotl/deepspeed_configs/zero3_bf16.json
gradient_checkpointing: true

strict: false
local_rank:
